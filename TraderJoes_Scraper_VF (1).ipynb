{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956f185b-6a6d-4fb4-92dc-93145f58bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 111: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A111%7D\n",
      "Scraping page 112: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A112%7D\n",
      "Scraping page 113: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A113%7D\n",
      "Scraping page 114: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A114%7D\n",
      "Scraping page 115: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A115%7D\n",
      "Scraping page 116: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A116%7D\n",
      "Scraping page 117: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A117%7D\n",
      "Scraping page 118: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A118%7D\n",
      "Scraping page 119: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A119%7D\n",
      "Scraping page 120: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A120%7D\n",
      "Scraping page 121: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A121%7D\n",
      "Scraping page 122: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A122%7D\n",
      "Scraping page 123: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A123%7D\n",
      "Scraping page 124: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A124%7D\n",
      "Scraping page 125: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A125%7D\n",
      "Scraping page 126: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A126%7D\n",
      "Scraping page 127: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A127%7D\n",
      "Scraping page 128: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A128%7D\n",
      "Scraping page 129: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A129%7D\n",
      "Scraping page 130: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A130%7D\n",
      "Scraping page 131: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A131%7D\n",
      "Scraping page 132: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A132%7D\n",
      "Scraping page 133: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A133%7D\n",
      "Scraping page 134: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A134%7D\n",
      "Scraping page 135: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A135%7D\n",
      "Scraping page 136: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A136%7D\n",
      "Scraping page 137: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A137%7D\n",
      "Scraping page 138: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A138%7D\n",
      "Scraping page 139: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A139%7D\n",
      "Scraping page 140: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A140%7D\n",
      "Scraping page 141: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A141%7D\n",
      "Scraping page 142: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A142%7D\n",
      "Scraping page 143: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A143%7D\n",
      "Scraping page 144: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A144%7D\n",
      "Scraping page 145: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A145%7D\n",
      "Scraping page 146: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A146%7D\n",
      "Scraping page 147: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A147%7D\n",
      "Scraping page 148: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A148%7D\n",
      "Scraping page 149: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A149%7D\n",
      "Scraping page 150: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A150%7D\n",
      "Scraping page 151: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A151%7D\n",
      "Scraping page 152: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A152%7D\n",
      "Scraping page 153: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A153%7D\n",
      "Scraping page 154: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A154%7D\n",
      "Scraping page 155: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A155%7D\n",
      "Scraping page 156: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A156%7D\n",
      "Scraping page 157: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A157%7D\n",
      "Scraping page 158: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A158%7D\n",
      "Scraping page 159: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A159%7D\n",
      "Scraping page 160: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A160%7D\n",
      "Scraping page 161: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A161%7D\n",
      "Scraping page 162: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A162%7D\n",
      "Scraping page 163: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A163%7D\n",
      "Scraping page 164: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A164%7D\n",
      "Scraping page 165: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A165%7D\n",
      "Scraping page 166: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A166%7D\n",
      "Scraping page 167: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A167%7D\n",
      "Scraping page 168: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A168%7D\n",
      "Scraping page 169: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A169%7D\n",
      "Scraping page 170: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A170%7D\n",
      "Scraping page 171: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A171%7D\n",
      "Scraping page 172: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A172%7D\n",
      "Scraping page 173: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A173%7D\n",
      "Scraping page 174: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A174%7D\n",
      "Scraping page 175: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A175%7D\n",
      "Scraping page 176: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A176%7D\n",
      "Scraping page 177: https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A177%7D\n",
      "Timeout encountered on page 177. Stopping further scraping.\n",
      "Scraping complete. Data appended to trader_joes_products.csv\n"
     ]
    }
   ],
   "source": [
    "#!pip install selenium\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_traderjoes_page(url):\n",
    "    \"\"\"\n",
    "    Scrapes a single Trader Joe's food category page.\n",
    "    Returns a list of dictionaries with keys 'name' and 'url'.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # allow initial scripts to run\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Repeat scrolling to load all items\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        # Wait for product list\n",
    "        locator = (By.CSS_SELECTOR, \"li[class^='ProductList_productList__item__']\")\n",
    "        product_items = WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_all_elements_located(locator)\n",
    "        )\n",
    "\n",
    "        for item in product_items:\n",
    "            try:\n",
    "                # Finds product name in a <h2><a ...> element\n",
    "                link_elem = item.find_element(By.CSS_SELECTOR, \"h2 > a\")\n",
    "                product_name = link_elem.text.strip()\n",
    "                product_href = link_elem.get_attribute(\"href\")\n",
    "                results.append({\n",
    "                    \"name\": product_name,\n",
    "                    \"url\": product_href\n",
    "                })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.traderjoes.com/home/products/category/food-8\"\n",
    "    all_products = []\n",
    "\n",
    "    # Loop through pages in index\n",
    "    for page in range(1, 200):\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"https://www.traderjoes.com/home/products/category/food-8?filters=%7B%22page%22%3A{page}%7D\"\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        try:\n",
    "            page_products = scrape_traderjoes_page(url)\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout encountered on page {page}. Stopping further scraping.\")\n",
    "            break\n",
    "        all_products.extend(page_products)\n",
    "\n",
    "    # Append the collected product data to CSV file\n",
    "    csv_filename = \"trader_joes_products_TEST.csv\"\n",
    "    file_exists = os.path.exists(csv_filename)\n",
    "    with open(csv_filename, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=[\"name\", \"url\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(all_products)\n",
    "\n",
    "    print(f\"Scraping complete. Data appended to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c71d2b-1d1b-4360-9307-0259269165b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pandas in /srv/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /srv/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /srv/conda/lib/python3.11/site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /srv/conda/lib/python3.11/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /srv/conda/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /srv/conda/lib/python3.11/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /srv/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /srv/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /srv/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /srv/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /srv/conda/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Using cached selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
      "Using cached trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.29.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "Scraping product at index 1751: https://www.traderjoes.com/home/products/pdp/organic-acai-bowl-064672\n",
      "Scraping complete. Data appended to NEW_trader_joes_product_details.csv\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium pandas\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def extract_numeric(value):\n",
    "    \"\"\"Extract a numeric value from a string whenever possible.\"\"\"\n",
    "    match = re.search(r\"[\\d.]+\", value)\n",
    "    if match:\n",
    "        try:\n",
    "            return float(match.group())\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def scrape_traderjoes_product(url):\n",
    "    \"\"\"\n",
    "    Scrapes a Trader Joe's product page and returns a dictionary with:\n",
    "      - name, price, package_size, serving_size, calories_per_serving\n",
    "      - serves_about, number of servings\n",
    "      - each nutrient from the nutrition table as its own key.\n",
    "    If multiple values are found for the same nutrient, the smallest numeric value is kept.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    product_data = {}\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Let page load initial scripts\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # 1) Attempt to select the \"Per Serving\" tab\n",
    "        try:\n",
    "            tab_bar = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class^='TabBar_tabBar__nav__']\"))\n",
    "            )\n",
    "            buttons = tab_bar.find_elements(By.TAG_NAME, \"button\")\n",
    "            for btn in buttons:\n",
    "                text_content = btn.get_attribute(\"textContent\").strip().lower()\n",
    "                if \"per serving\" in text_content:\n",
    "                    # Only click if not already active\n",
    "                    if \"Nav_active__\" not in btn.get_attribute(\"class\"):\n",
    "                        btn.click()\n",
    "                        time.sleep(1)\n",
    "                    break\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "\n",
    "        # 2) Scrape product name\n",
    "        try:\n",
    "            title_elem = driver.find_element(By.CSS_SELECTOR, \"h1[class^='ProductDetails_main__title__']\")\n",
    "            product_data[\"name\"] = title_elem.get_attribute(\"textContent\").strip()\n",
    "        except Exception:\n",
    "            product_data[\"name\"] = \"\"\n",
    "        \n",
    "        # 3) Scrape price\n",
    "        try:\n",
    "            price_elem = driver.find_element(By.CSS_SELECTOR, \"span[class^='ProductPrice_productPrice__price__']\")\n",
    "            product_data[\"price\"] = price_elem.get_attribute(\"textContent\").strip()\n",
    "        except Exception:\n",
    "            product_data[\"price\"] = \"\"\n",
    "        \n",
    "        # 4) Scrape package size\n",
    "        try:\n",
    "            size_elem = driver.find_element(By.CSS_SELECTOR, \"span[class^='ProductPrice_productPrice__unit__']\")\n",
    "            product_data[\"package_size\"] = size_elem.get_attribute(\"textContent\").strip()\n",
    "        except Exception:\n",
    "            product_data[\"package_size\"] = \"\"\n",
    "        \n",
    "        # 5) Scrape serving size & calories per serving\n",
    "        serving_size = \"\"\n",
    "        calories_per_serving = \"\"\n",
    "        try:\n",
    "            characteristics = driver.find_elements(By.CSS_SELECTOR, \"div[class^='Item_characteristics__item__']\")\n",
    "            for block in characteristics:\n",
    "                try:\n",
    "                    label_div = block.find_element(By.CSS_SELECTOR, \"div[class^='Item_characteristics__title__']\")\n",
    "                    value_div = block.find_element(By.CSS_SELECTOR, \"div[class^='Item_characteristics__text__']\")\n",
    "                    label_text = label_div.get_attribute(\"textContent\").strip().lower()\n",
    "                    value_text = value_div.get_attribute(\"textContent\").strip()\n",
    "                    if label_text == \"serving size\":\n",
    "                        serving_size = value_text\n",
    "                    elif label_text == \"calories per serving\":\n",
    "                        calories_per_serving = value_text\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "        product_data[\"serving_size\"] = serving_size\n",
    "        product_data[\"calories_per_serving\"] = calories_per_serving\n",
    "\n",
    "        # 6) Scrape \"Serves about\" text by scanning table rows for a <th> that has \"serves\" in it\n",
    "        serves_about = \"\"\n",
    "        try:\n",
    "            table_rows = driver.find_elements(By.CSS_SELECTOR, \"tr[class^='Item_table__row__']\")\n",
    "            for row in table_rows:\n",
    "                th_elems = row.find_elements(By.TAG_NAME, \"th\")\n",
    "                for th in th_elems:\n",
    "                    text_val = th.get_attribute(\"textContent\").strip()\n",
    "                    # Only rely on the substring \"serves\"\n",
    "                    if \"serves\" in text_val.lower():\n",
    "                        serves_about = text_val\n",
    "                        break\n",
    "                if serves_about:\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "        product_data[\"serves_about\"] = serves_about\n",
    "\n",
    "        # 7) Extract nutrition table rows, storing smallest numeric values if duplicates appear\n",
    "        nutrition = {}\n",
    "        try:\n",
    "            row_elements = driver.find_elements(By.CSS_SELECTOR, \"tr[class^='Item_table__row__']\")\n",
    "            for row in row_elements:\n",
    "                cells = row.find_elements(By.CSS_SELECTOR, \"td[class^='Item_table__cell__']\")\n",
    "                if len(cells) >= 2:\n",
    "                    nutrient = cells[0].get_attribute(\"textContent\").strip()\n",
    "                    value = cells[1].get_attribute(\"textContent\").strip()\n",
    "                    if nutrient:\n",
    "                        if nutrient in nutrition:\n",
    "                            old_val = nutrition[nutrient]\n",
    "                            new_num = extract_numeric(value)\n",
    "                            old_num = extract_numeric(old_val)\n",
    "                            if new_num is not None and old_num is not None:\n",
    "                                # Keep the smaller numeric value\n",
    "                                if new_num < old_num:\n",
    "                                    nutrition[nutrient] = value\n",
    "                            elif new_num is not None:\n",
    "                                # If old value wasn't numeric but the new one is, override\n",
    "                                nutrition[nutrient] = value\n",
    "                            # If new_num is None, do nothing\n",
    "                        else:\n",
    "                            nutrition[nutrient] = value\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Merge nutrition fields into product data\n",
    "        product_data.update(nutrition)\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return product_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Reads CSV with product URLs.\n",
    "    urls_csv = \"trader_joes_products.csv\"\n",
    "    urls_df = pd.read_csv(urls_csv)\n",
    "    \n",
    "    # Range of products to scrape (by index)\n",
    "    start_index = 1\n",
    "    end_index = 1754\n",
    "    subset_df = urls_df.iloc[start_index:end_index]\n",
    "    \n",
    "    # List to store scraped product details.\n",
    "    details_list = []\n",
    "    \n",
    "    for idx, row in subset_df.iterrows():\n",
    "        url = row[\"url\"]\n",
    "        print(f\"Scraping product at index {idx}: {url}\")\n",
    "        try:\n",
    "            product_details = scrape_traderjoes_product(url)\n",
    "            details_list.append(product_details)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "    \n",
    "    # Convert scraped details to a DataFrame.\n",
    "    new_details_df = pd.DataFrame(details_list)\n",
    "    \n",
    "    # Merge with existing product details (if any), to account for new columns\n",
    "    details_csv = \"NEW_trader_joes_product_details.csv\"\n",
    "    try:\n",
    "        existing_df = pd.read_csv(details_csv)\n",
    "        combined_df = pd.concat([existing_df, new_details_df], ignore_index=True, sort=False)\n",
    "    except FileNotFoundError:\n",
    "        combined_df = new_details_df\n",
    "    \n",
    "    # Write the updated data back to the CSV.\n",
    "    combined_df.to_csv(details_csv, index=False)\n",
    "    print(f\"Scraping complete. Data appended to {details_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0883842-1264-4673-b893-2a09634736e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
